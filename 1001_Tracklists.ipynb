{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1001 Tracklists Scraper\n",
    "=======================\n",
    "A set of functions to scrape music tracklists from [1001 Tracklists](https://www.1001tracklists.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import a bunch of stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests \n",
    "import pandas as pd\n",
    "import urllib3\n",
    "import os\n",
    "import spotipy\n",
    "import spotipy.util as util\n",
    "from pprint import pprint\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get track data from spotify and return it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id='6389b29d73fc4806ba5e812e678854c1'\n",
    "client_secret='0b4bcd832e694aedad408b5b4a93dd5c'\n",
    "ccm=util.oauth2.SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)\n",
    "sp =spotipy.Spotify(client_credentials_manager=ccm)\n",
    "\n",
    "def get_attrs(artist, track):\n",
    "    try:\n",
    "        res = sp.search(q='artist:'+artist+' track:'+track, type=\"track\")\n",
    "        track_res = res['tracks']['items'][0]\n",
    "        track_id = track_res['uri']\n",
    "        deets = sp.audio_features(track_id)\n",
    "        return pd.Series(deets[0])\n",
    "    except Exception as e:\n",
    "        return False\n",
    "    \n",
    "#get_attrs(artist='Armin Van Buuren', track=\"shivers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_camelot(key, mode):\n",
    "    # index of letter is spotify pitch class - e.g. 0->c, 1->c# etc.\n",
    "    tones = [ 'c', 'c#', 'd', 'd#', 'e', 'f', 'f#', 'g', 'g#', 'a', 'a#', 'b' ]\n",
    "    keys = {\n",
    "        'a0': '8A',\n",
    "        'a1': '11B',\n",
    "        'a#0': '3A',\n",
    "        'a#1': '6B',\n",
    "        'b0': '10A',\n",
    "        'b1': '1B',\n",
    "        'c0': '5A',\n",
    "        'c1': '8B',\n",
    "        'c#0': '12A',\n",
    "        'c#1': '3B',\n",
    "        'd0': '7A',\n",
    "        'd1': '10B',\n",
    "        'd#0': '2A',\n",
    "        'd#1': '5B',\n",
    "        'e0': '9A',\n",
    "        'e1': '12B',\n",
    "        'f0': '4A',\n",
    "        'f1': '7B',\n",
    "        'f#0': '11A',\n",
    "        'f#1': '2B',\n",
    "        'g0': '6A',\n",
    "        'g1': '9B',\n",
    "        'g#0': '1A',\n",
    "        'g#1': '4B',\n",
    "    }\n",
    "    key_letter = tones[key]\n",
    "    return keys[key_letter+str(mode)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a tracklist from 1001 Tracklists and write it to a CSV with Spotify track info for all songs it can find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tracklist(url, folder='.'):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    \n",
    "    !wget {url} -q\n",
    "    fname = url.split('/')[-1]\n",
    "    #print(\"Fname: \"+fname)\n",
    "    soup = bs(open(fname), \"lxml\")\n",
    "    !rm {fname}\n",
    "    \n",
    "    tracklist = pd.DataFrame(columns=['Artist(s)', 'Title', 'Release'])\n",
    "    set_name = soup.find(id=\"pageTitle\").get_text().strip()\n",
    "    print(set_name)\n",
    "    \n",
    "    for div in soup.select('.trackValue'):\n",
    "        try:\n",
    "            text = div.get_text()\n",
    "            artists_raw = text.split('-')[0].split(' & ')\n",
    "            artists = \" \".join(artists_raw) \n",
    "            artists = artists.replace('vs.', ' ').strip()\n",
    "            artists = artists.replace('ft.', ' ')\n",
    "            #print(\"Artists: \"+artists)\n",
    "            \n",
    "            # title after '-' but before both label ([) and release (() and if a mashup, remove all but first song to make it easier to look up\n",
    "            title = text.split('-')[1].split('[')[0].split('(')[0].split('vs.')[0]\n",
    "            #print(\"title: \"+title)\n",
    "            \n",
    "            # Releases in braces\n",
    "            try:\n",
    "                release = text.split('(')[1].split(')')[0]\n",
    "            except:\n",
    "                release = \"unknown\"\n",
    "            #print(\"release: \"+release)\n",
    "            \n",
    "            # Label in []\n",
    "            try:\n",
    "                label = text.split('[')[1].split(']')[0]\n",
    "            except:\n",
    "                label = \"uknown\"\n",
    "\n",
    "            basic_details = pd.Series([artists, title, release], index=['Artist(s)', 'Title', 'Release'])\n",
    "            \n",
    "            #don't search spotify for unknown version, just search base name\n",
    "            if release == 'unknown':\n",
    "                release = ''\n",
    "                \n",
    "            spotify_details = get_attrs(artist=artists+' '+release, track=title)\n",
    "            # Try removing version if not found\n",
    "            if spotify_details is False:\n",
    "                for a in artists.split(' '):\n",
    "                    spotify_details = get_attrs(artist=a, track=title)\n",
    "                    if spotify_details is not False:\n",
    "                        break\n",
    "            # Set details to none instead of false if not found, so track won't get excluded\n",
    "            if spotify_details is False:\n",
    "                spotify_details = None\n",
    "                print(f'Not Found: {title} by {artists}')\n",
    "            row = pd.concat([basic_details, spotify_details])\n",
    "            tracklist = tracklist.append(row, ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    tracklist.to_csv(f'{folder}/{set_name}.csv')\n",
    "    \n",
    "#get_tracklist(url='https://www.1001tracklists.com/tracklist/1kjuxf4t/giuseppe-ottaviani-go-on-air-fsoe-stage-tomorrowland-belgium-2018-08-21.html', folder='otaviani_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a whole series of tracklists, and put them in a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_tracklists(series_url, folder='.', recursecall=False):\n",
    "    fname=series_url.split('/')[-1]\n",
    "    doesnt_exist = !ls | grep fname\n",
    "    if doesnt_exist != 0:\n",
    "        !wget {series_url} \n",
    "        soup = bs(open(fname), \"lxml\")\n",
    "        !rm {fname}\n",
    "    else:\n",
    "        print(fname+\" already exists, skipping...\")\n",
    "    main = soup.find(id='mainContentDiv')\n",
    "    for mix_link in main.find_all('a', href=True):\n",
    "        mix_href = mix_link['href']\n",
    "        if mix_href.startswith('/tracklist/'):\n",
    "            webpage = '/'.join(series_url.split('/')[0:3])\n",
    "            get_tracklist(webpage+mix_link['href'], folder=folder)\n",
    "        \n",
    "    if recursecall == False:\n",
    "        page_div = soup.find('ul', class_='pagination')\n",
    "        other_pages = page_div.find_all('a', href=True)\n",
    "        dont_follow = ['Prev', '1', '2', '3', 'Next']\n",
    "        cur_page_num = int(series_url.split('/')[-1].split('.')[-2][-1])\n",
    "        for page in other_pages:\n",
    "            try:\n",
    "                page_num = int(page.get_text())\n",
    "            except:\n",
    "                continue\n",
    "            if page_num > cur_page_num:\n",
    "                group = series_url[0:series_url.rfind('/')]\n",
    "                print(group)\n",
    "                get_series_tracklists(group+'/'+page['href'], folder=folder, recursecall=True)\n",
    "                \n",
    "get_series_tracklists(series_url='https://www.1001tracklists.com/groups/nlzzgw/evans-picks/index9.html', folder='Evans_Picks')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
