{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1001 Tracklists Scraper\n",
    "=======================\n",
    "A set of functions to scrape music tracklists from [1001 Tracklists](https://www.1001tracklists.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import a bunch of stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests \n",
    "import pandas as pd\n",
    "import urllib3\n",
    "import os\n",
    "import spotipy\n",
    "import spotipy.util as util\n",
    "from pprint import pprint\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get track data from spotify and return it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id='6389b29d73fc4806ba5e812e678854c1'\n",
    "client_secret='0b4bcd832e694aedad408b5b4a93dd5c'\n",
    "ccm=util.oauth2.SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)\n",
    "sp =spotipy.Spotify(client_credentials_manager=ccm)\n",
    "\n",
    "def get_attrs(artist, track):\n",
    "    try:\n",
    "        res = sp.search(q='artist:'+artist+' track:'+track, type=\"track\")\n",
    "        track_res = res['tracks']['items'][0]\n",
    "        track_id = track_res['uri']\n",
    "        deets = sp.audio_features(track_id)\n",
    "        return pd.Series(deets[0])\n",
    "    except Exception as e:\n",
    "        return False\n",
    "    \n",
    "#get_attrs(artist='Armin Van Buuren', track=\"shivers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a tracklist from 1001 Tracklists and write it to a CSV with Spotify track info for all songs it can find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tracklist(url, folder='.'):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    \n",
    "    !wget {url} -q\n",
    "    fname = url.split('/')[-1]\n",
    "    #print(\"Fname: \"+fname)\n",
    "    soup = bs(open(fname), \"lxml\")\n",
    "    !rm {fname}\n",
    "    \n",
    "    tracklist = pd.DataFrame(columns=['Artist(s)', 'Title', 'Release'])\n",
    "    set_name = soup.find(id=\"pageTitle\").get_text()\n",
    "    print(set_name)\n",
    "    \n",
    "    for div in soup.select('.trackValue'):\n",
    "        try:\n",
    "            text = div.get_text()\n",
    "            artists_raw = text.split('-')[0].split(' & ')\n",
    "            artists = \" \".join(artists_raw) \n",
    "            artists = artists.replace('vs.', ' ').strip()\n",
    "            artists = artists.replace('ft.', ' ')\n",
    "            #print(\"Artists: \"+artists)\n",
    "            \n",
    "            # title after '-' but before both label ([) and release (() and if a mashup, remove all but first song to make it easier to look up\n",
    "            title = text.split('-')[1].split('[')[0].split('(')[0].split('vs.')[0]\n",
    "            #print(\"title: \"+title)\n",
    "            \n",
    "            # Releases in braces\n",
    "            try:\n",
    "                release = text.split('(')[1].split(')')[0]\n",
    "            except:\n",
    "                release = \"unknown\"\n",
    "            #print(\"release: \"+release)\n",
    "            \n",
    "            # Label in []\n",
    "            try:\n",
    "                label = text.split('[')[1].split(']')[0]\n",
    "            except:\n",
    "                label = \"uknown\"\n",
    "\n",
    "            basic_details = pd.Series([artists, title, release], index=['Artist(s)', 'Title', 'Release'])\n",
    "            \n",
    "            #don't search spotify for unknown version, just search base name\n",
    "            if release == 'unknown':\n",
    "                release = ''\n",
    "                \n",
    "            spotify_details = get_attrs(artist=artists+' '+release, track=title)\n",
    "            # Try removing version if not found\n",
    "            if spotify_details is False:\n",
    "                for a in artists.split(' '):\n",
    "                    spotify_details = get_attrs(artist=a, track=title)\n",
    "                    if spotify_details is not False:\n",
    "                        break\n",
    "            # Set details to none instead of false if not found, so track won't get excluded\n",
    "            if spotify_details is False:\n",
    "                spotify_details = None\n",
    "                print(f'Not Found: {title} by {artists}')\n",
    "            row = pd.concat([basic_details, spotify_details])\n",
    "            tracklist = tracklist.append(row, ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    tracklist.to_csv(f'{folder}/{set_name}.csv')\n",
    "    \n",
    "#get_tracklist(url='https://www.1001tracklists.com/tracklist/1kjuxf4t/giuseppe-ottaviani-go-on-air-fsoe-stage-tomorrowland-belgium-2018-08-21.html', folder='otaviani_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a whole series of tracklists, and put them in a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-10-09 10:58:41--  https://www.1001tracklists.com/groups/8pmz4n/exclusive-1001tl-mixes/index6.html\n",
      "Resolving www.1001tracklists.com (www.1001tracklists.com)... 158.69.5.7\n",
      "Connecting to www.1001tracklists.com (www.1001tracklists.com)|158.69.5.7|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘index6.html’\n",
      "\n",
      "index6.html             [  <=>               ] 129.92K   546KB/s    in 0.2s    \n",
      "\n",
      "2018-10-09 10:58:42 (546 KB/s) - ‘index6.html’ saved [133037]\n",
      "\n",
      "fname: index6.html\n",
      "/tracklist/243dfw39/cid-1001tracklists-exclusive-mix-2017-02-03.html\n",
      " CID - 1001Tracklists Exclusive Mix 2017-02-03 \n",
      "/tracklist/2m06tdnt/karim-mika-1001tracklists-exclusive-mix-2017-01-30.html\n",
      " Karim Mika - 1001Tracklists Exclusive Mix 2017-01-30 \n",
      "Not Found:  Yummy   by Faruk Sabanci\n",
      "Not Found:  Closer Crysis  by Chainsmokers   Sunstars\n",
      "/tracklist/1tvk9vzt/point-blvnk-1001tracklists-exclusive-mix-2017-01-26.html\n",
      " POINT BLVNK - 1001Tracklists Exclusive Mix 2017-01-26 \n",
      "Not Found:  This Is How We Party   by ID\n",
      "Not Found:  Can't Stop   by YDER\n",
      "/tracklist/2mxzy151/elephante-1001tracklists-exclusive-mix-2017-01-23.html\n",
      " Elephante - 1001Tracklists Exclusive Mix 2017-01-23 \n",
      "Not Found:  Ghosts N' Sharks   by JAUZ x Ghastly\n",
      "Not Found:  Say Things  by graves Coolights\n",
      "/tracklist/nmfsxpt/kshmr-1001tracklists-exclusive-mix-2017-01-19.html\n",
      " KSHMR - 1001Tracklists Exclusive Mix 2017-01-19 \n",
      "Not Found:  Tarana   by Jayden Jaxx Hasit Nanda   Mitika\n"
     ]
    }
   ],
   "source": [
    "def get_series_tracklists(series_url, folder='.', recursecall=False):\n",
    "    !wget {series_url} \n",
    "    fname=series_url.split('/')[-1]\n",
    "    print(\"fname: \"+fname)\n",
    "    soup = bs(open(fname), \"lxml\")\n",
    "    !rm {fname}\n",
    "    main = soup.find(id='mainContentDiv')\n",
    "    for mix_link in main.find_all('a', href=True):\n",
    "        mix_href = mix_link['href']\n",
    "        if mix_href.startswith('/tracklist/'):\n",
    "            webpage = '/'.join(series_url.split('/')[0:3])\n",
    "            print(mix_href)\n",
    "            #print(soup.find('body').prettify())\n",
    "            get_tracklist(webpage+mix_link['href'], folder=folder)\n",
    "        \n",
    "    if recursecall == False:\n",
    "        page_div = soup.find('ul', class_='pagination')\n",
    "        other_pages = page_div.find_all('a', href=True)\n",
    "        dont_follow = ['Prev', '1', '2', '3', '4', '5', 'Next']\n",
    "        for page in other_pages:\n",
    "            if page.get_text() not in dont_follow: #and mix_href != '/info/cookies.html':\n",
    "                group = series_url[0:series_url.rfind('/')]\n",
    "                print(group)\n",
    "                get_series_tracklists(group+'/'+page['href'], folder=folder, recursecall=True)\n",
    "                \n",
    "get_series_tracklists(series_url='https://www.1001tracklists.com/groups/8pmz4n/exclusive-1001tl-mixes/index6.html', folder='1001-exclusives')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
