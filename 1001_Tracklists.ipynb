{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1001 Tracklists Scraper\n",
    "=======================\n",
    "A set of functions to scrape music tracklists from [1001 Tracklists](https://www.1001tracklists.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import a bunch of stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests \n",
    "import pandas as pd\n",
    "import urllib3\n",
    "import os\n",
    "import spotipy\n",
    "import spotipy.util as util\n",
    "from pprint import pprint\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_url = 'https://www.1001tracklists.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get track data from spotify and return it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id='6389b29d73fc4806ba5e812e678854c1'\n",
    "client_secret='0b4bcd832e694aedad408b5b4a93dd5c'\n",
    "ccm=util.oauth2.SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)\n",
    "sp =spotipy.Spotify(client_credentials_manager=ccm)\n",
    "\n",
    "def get_attrs(artist, track):\n",
    "    try:\n",
    "        q =\"artist:\"+artist+\" track:\"+track\n",
    "        res = sp.search(q=q, type=\"track\")\n",
    "        track_res = res['tracks']['items'][0]\n",
    "        track_id = track_res['uri']\n",
    "        deets = sp.audio_features(track_id)\n",
    "        camelot = get_camelot(deets[0]['key'], deets[0]['mode'])\n",
    "        deets[0]['camelot'] = camelot\n",
    "        return pd.Series(deets[0])\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        return False\n",
    "    \n",
    "#get_attrs(artist='Armin Van Buuren', track=\"shivers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_camelot(key, mode):\n",
    "    # index of letter is spotify pitch class - e.g. 0->c, 1->c# etc.\n",
    "    tones = [ 'c', 'c#', 'd', 'd#', 'e', 'f', 'f#', 'g', 'g#', 'a', 'a#', 'b' ]\n",
    "    keys = {\n",
    "        'a0': '8A',\n",
    "        'a1': '11B',\n",
    "        'a#0': '3A',\n",
    "        'a#1': '6B',\n",
    "        'b0': '10A',\n",
    "        'b1': '1B',\n",
    "        'c0': '5A',\n",
    "        'c1': '8B',\n",
    "        'c#0': '12A',\n",
    "        'c#1': '3B',\n",
    "        'd0': '7A',\n",
    "        'd1': '10B',\n",
    "        'd#0': '2A',\n",
    "        'd#1': '5B',\n",
    "        'e0': '9A',\n",
    "        'e1': '12B',\n",
    "        'f0': '4A',\n",
    "        'f1': '7B',\n",
    "        'f#0': '11A',\n",
    "        'f#1': '2B',\n",
    "        'g0': '6A',\n",
    "        'g1': '9B',\n",
    "        'g#0': '1A',\n",
    "        'g#1': '4B',\n",
    "    }\n",
    "    key_letter = tones[key]\n",
    "    return keys[key_letter+str(mode)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a tracklist from 1001 Tracklists and write it to a CSV with Spotify track info for all songs it can find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsplit(s, sep):\n",
    "    stack = [s]\n",
    "    for char in sep:\n",
    "        pieces = []\n",
    "        for substr in stack:\n",
    "            pieces.extend(substr.split(char))\n",
    "        stack = pieces\n",
    "    return stack\n",
    "\n",
    "def get_tracklist(url, folder='.'):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    \n",
    "    !wget {url} -q\n",
    "    fname = url.split('/')[-1]\n",
    "    #print(\"Fname: \"+fname)\n",
    "    soup = bs(open(fname), \"lxml\")\n",
    "    !rm {fname}\n",
    "    \n",
    "    tracklist = pd.DataFrame(columns=['Artist(s)', 'Title', 'Release'])\n",
    "    set_name = soup.find(id=\"pageTitle\").get_text().strip()\n",
    "    print(set_name)\n",
    "    \n",
    "    for div in soup.select('.trackValue'):\n",
    "        try:\n",
    "            text = div.get_text()\n",
    "            artist_part = text.split('-')[0]\n",
    "            artists_raw = tsplit(artist_part, ('&','vs.', 'ft.', 'pres.'))\n",
    "            for index in range(len(artists_raw)):\n",
    "                artists_raw[index] = artists_raw[index].strip()\n",
    "            artists = \",\".join(artists_raw)\n",
    "            #print(\"Artists: \"+artists)\n",
    "            \n",
    "            # title after '-' but before both label ([) and release (() and if a mashup, remove all but first song to make it easier to look up\n",
    "            title = text.split('-')[1].split('[')[0].split('(')[0].split('vs.')[0].strip()\n",
    "            #print(\"title: \"+title)\n",
    "            \n",
    "            # Releases in braces\n",
    "            try:\n",
    "                release = text.split('(')[1].split(')')[0].strip()\n",
    "            except:\n",
    "                release = \"unknown\"\n",
    "            #print(\"release: \"+release)\n",
    "            \n",
    "            # Label in []\n",
    "            try:\n",
    "                label = text.split('[')[1].split(']')[0].strip()\n",
    "            except:\n",
    "                label = \"uknown\"\n",
    "\n",
    "            basic_details = pd.Series([artists, title, release], index=['Artist(s)', 'Title', 'Release'])\n",
    "            \n",
    "            #don't search spotify for unknown version, just search base name\n",
    "            if release == 'unknown':\n",
    "                release = ''\n",
    "                \n",
    "            spaced_artists = artists.replace(',', ' ')\n",
    "            spotify_details = get_attrs(artist=spaced_artists+' '+release, track=title)\n",
    "            # Try removing version if not found\n",
    "            if spotify_details is False:\n",
    "                for a in artists.split(','):\n",
    "                    spotify_details = get_attrs(artist=a, track=title)\n",
    "                    #print(f'Trying again with artist: \"{a}\" title: \"{title}\"')\n",
    "                    if spotify_details is not False:\n",
    "                        break\n",
    "            # Set details to none instead of false if not found, so track won't get excluded\n",
    "            if spotify_details is False:\n",
    "                spotify_details = None\n",
    "                print(f'Not Found: \"{title}\" by \"{artists}\"')\n",
    "            row = pd.concat([basic_details, spotify_details])\n",
    "            tracklist = tracklist.append(row, ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    tracklist.to_csv(f'{folder}/{set_name}.csv')\n",
    "    \n",
    "#get_tracklist(url='https://www.1001tracklists.com/tracklist/1kjuxf4t/giuseppe-ottaviani-go-on-air-fsoe-stage-tomorrowland-belgium-2018-08-21.html', folder='otaviani_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a whole series of tracklists, and put them in a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_tracklists(series_url, folder='.', recursecall=False):\n",
    "    fname=series_url.split('/')[-1]\n",
    "    doesnt_exist = !ls | grep fname\n",
    "    if doesnt_exist != 0:\n",
    "        !wget {series_url} -q\n",
    "        soup = bs(open(fname), \"lxml\")\n",
    "        !rm {fname}\n",
    "    else:\n",
    "        print(fname+\" already exists, skipping...\")\n",
    "    main = soup.find(id='mainContentDiv')\n",
    "    for mix_link in main.find_all('a', href=True):\n",
    "        mix_href = mix_link['href']\n",
    "        if mix_href.startswith('/tracklist/'):\n",
    "            webpage = '/'.join(series_url.split('/')[0:3])\n",
    "            get_tracklist(webpage+mix_link['href'], folder=folder)\n",
    "        \n",
    "    if recursecall == False:\n",
    "        page_div = soup.find('ul', class_='pagination')\n",
    "        other_pages = page_div.find_all('a', href=True)\n",
    "        dont_follow = ['Prev', '1', 'Next']\n",
    "        try:\n",
    "            cur_page_num = int(series_url.split('/')[-1].split('.')[-2][-1])\n",
    "        except:\n",
    "            cur_page_num = 1\n",
    "        for page in other_pages:\n",
    "            try:\n",
    "                page_num = int(page.get_text())\n",
    "            except:\n",
    "                continue\n",
    "            if page_num > cur_page_num:\n",
    "                group = series_url[0:series_url.rfind('/')]\n",
    "                print(group)\n",
    "                get_series_tracklists(group+'/'+page['href'], folder=folder, recursecall=True)\n",
    "                \n",
    "#get_series_tracklists(series_url='https://www.1001tracklists.com/groups/nlzzgw/evans-picks/index9.html', folder='Evans_Picks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amsterdam Dance Event 2018\n",
      "Series exists, skipping\n",
      "Deep Progressive Techhouse Sets\n",
      "Series exists, skipping\n",
      "HEXAGON HQ\n",
      "Series exists, skipping\n",
      "Unity Brothers Podcast by Unity Brothers\n",
      "Series exists, skipping\n",
      "Evan's Picks\n",
      "Series exists, skipping\n",
      "SPINNIN' Promo Mixes\n",
      "Series exists, skipping\n",
      "Tiesto Tracklists Live\n",
      "Series exists, skipping\n",
      "VillaHangar Music In The Air Podcast Show\n",
      "Series exists, skipping\n",
      "Exclusive 1001TL Mixes\n",
      "Series exists, skipping\n",
      "Tomorrowland 2018\n",
      "Series exists, skipping\n",
      "Tomorrowland Belgium 2018\n",
      "Series exists, skipping\n",
      "Sensation Megamixes\n",
      "Series exists, skipping\n",
      "Source Recordings Seasonal Mixes\n",
      "Series exists, skipping\n",
      "Axtone Presents\n",
      "Series exists, skipping\n",
      "Deep House DJ SETS - LOCAL & ABROAD\n",
      "Series exists, skipping\n",
      "Aftermovies\n",
      "Series exists, skipping\n",
      "Hard Dance\n",
      "Series exists, skipping\n",
      "Yearmixes 2017\n",
      "Series exists, skipping\n",
      "Finished DnB Tracklists\n",
      "Series exists, skipping\n",
      "Time Records Promo Mixes\n",
      "Series exists, skipping\n",
      "SIZE & Family\n",
      "Steve Angello & Danny Howard @ BBC Radio 1 Essential Mix (Creamfields UK, United Kingdom) 2018-09-01\n",
      "Not Found: \"Epileptic\" by \"Trace (UZ),Sebjak,Marcus Schossow,Florence + The Machine,Magnificence,Steve Angello\"\n",
      "Not Found: \"Epileptic\" by \"Trace (UZ)\"\n",
      "Not Found: \"Liceu\" by \"Sebjak,Marcus Schossow\"\n",
      "Not Found: \"For Sale\" by \"Buy Now\"\n",
      "Not Found: \"Don't You Worry Child\" by \"Swedish House Mafia,John Martin\"\n",
      "Not Found: \"Don't You Worry Child\" by \"Swedish House Mafia,John Martin\"\n",
      "Not Found: \"All About Love\" by \"Still Young,Steve Angello\"\n",
      "Not Found: \"You've Got The Love\" by \"Florence + The Machine\"\n",
      "Not Found: \"Women Of The Ghetto\" by \"Marlena Shaw\"\n",
      "Not Found: \"Can't Get Enough\" by \"Soulsearcher\"\n",
      "Matisse & Sadko - Monomark Radio 007 2018-04-09\n",
      "Not Found: \"Ōkami\" by \"Blinders\"\n",
      "Jose De Mara & Crusy - 1001Tracklists Exclusive Mix 2018-04-05\n",
      "Not Found: \"Cafe Del Mar\" by \"Hard Rock Sofa,Swanky Tunes\"\n",
      "Not Found: \"12 O'Clock\" by \"Corey James X Crusy,Jose De Mara\"\n",
      "Not Found: \"ID\" by \"Jose De Mara\"\n",
      "Kryder - Kryteria Radio 128 2018-04-04\n",
      "Not Found: \"Crypto Talks\" by \"Tom Sawyer\"\n",
      "Not Found: \"Cartagena\" by \"Eli Brown\"\n",
      "Matisse & Sadko - Monomark Radio 006 2018-04-02\n",
      "Tom Staar - STAAR PLAYR 003 2018-03-29\n",
      "Not Found: \"ID\" by \"Tom Staar\"\n",
      "Not Found: \"Chupala\" by \"Javi Reina,Mr. Sid\"\n",
      "Not Found: \"ID\" by \"Tom Staar\"\n",
      "Not Found: \"Don't You Worry Child\" by \"Swedish House Mafia,John Martin\"\n",
      "Kryder - Kryteria Radio 127 2018-03-28\n",
      "Not Found: \"Don't You Worry Child\" by \"Swedish House Mafia,John Martin\"\n",
      "Not Found: \"Mariachi\" by \"Tony Calrya,Snowx\"\n",
      "Matisse & Sadko - Monomark Radio 005 2018-03-26\n",
      "Kryder & Leandro Da Silva - Kryteria Radio 126 2018-03-21\n",
      "Not Found: \"Get Funky\" by \"Kryder\"\n",
      "Not Found: \"Tan Bueno\" by \"Gabry Venus X Georgia Mos\"\n",
      "Not Found: \"Needin' You\" by \"David Morales,The Face\"\n",
      "Not Found: \"ID\" by \"Leandro Da Silva\"\n",
      "Matisse & Sadko - Monomark Radio 004 2018-03-19\n",
      "Kryder - Kryteria Radio 125 2018-03-14\n",
      "Not Found: \"Why\" by \"L'Tric\"\n",
      "Matisse & Sadko - Monomark Radio 003 2018-03-12\n",
      "Not Found: \"Drix\" by \"Damien N\"\n",
      "Kryder - Kryteria Radio 124 2018-03-07\n",
      "Not Found: \"Reverse\" by \"ID\"\n",
      "Not Found: \"Big In Miami\" by \"Kryder\"\n",
      "Matisse & Sadko - Monomark Radio 002 2018-03-05\n",
      "Kryder - Kryteria Radio 123 2018-02-28\n",
      "Not Found: \"Seeline Woman\" by \"Nari,Benny Di Gioia\"\n",
      "Not Found: \"Chupala\" by \"Javi Reina,Mr. Sid\"\n",
      "Not Found: \"Are U Ready\" by \"Gianni Ruocco,DJ KK\"\n",
      "Matisse & Sadko - Monomark Radio 001 2018-02-26\n",
      "Nicky Romero & Corey James - Protocol Radio 289 2018-02-22\n",
      "Not Found: \"Visions\" by \"VY•DA x James South\"\n",
      "Tom Staar - STAAR PLAYR 002 2018-02-22\n",
      "Not Found: \"Papa Mondo\" by \"Tom Staar,Simon Kidzoo\"\n",
      "Not Found: \"East Soul\" by \"Trace (UZ)\"\n",
      "Not Found: \"Kasai\" by \"The Mambo Brothers\"\n",
      "Kryder & Antoine Delvig - Kryteria Radio 122 2018-02-21\n",
      "Not Found: \"Love Vibrations 2k18\" by \"Adrien Mezsi,Noizy Mark,Drop Department\"\n",
      "Not Found: \"I'll House You\" by \"Sunnery James,Ryan Marciano,Thomas Newson\"\n",
      "Not Found: \"22 ft. Medina\" by \"M\"\n",
      "Not Found: \"ID\" by \"Antoine Delvig\"\n",
      "Not Found: \"Watch The Apocalypse\" by \"Axwell,Steve Edwards,Arno Cost,Norman Doray\"\n",
      "Not Found: \"Qazar\" by \"DJ MERCER\"\n"
     ]
    }
   ],
   "source": [
    "def get_all_series(url, recursecall=False):\n",
    "    fname=url.split('/')[-1]\n",
    "    doesnt_exist = !ls | grep fname\n",
    "    if doesnt_exist != 0:\n",
    "        !wget {url} -q\n",
    "        soup = bs(open(fname), \"lxml\")\n",
    "        !rm {fname}\n",
    "    else:\n",
    "        print(\"already exists...\")\n",
    "    for series in soup.find_all('td', class_='tl'):\n",
    "        link = series.find('a', href=True)\n",
    "        href = link['href']\n",
    "        name = link.get_text()\n",
    "        print(name)\n",
    "        if os.path.exists(name):\n",
    "            print(\"Series exists, skipping\")\n",
    "            continue\n",
    "        get_series_tracklists(series_url=root_url+href, folder=name)\n",
    "    if recursecall == False:\n",
    "        page_div = soup.find('ul', class_='pagination')\n",
    "        other_pages = page_div.find_all('a', href=True)\n",
    "        dont_follow = ['Prev', '1', 'Next']\n",
    "        try:\n",
    "            cur_page_num = int(url.split('/')[-1].split('.')[-2][-1])\n",
    "        except:\n",
    "            cur_page_num = 1\n",
    "        for page in other_pages:\n",
    "            try:\n",
    "                page_num = int(page.get_text())\n",
    "            except:\n",
    "                continue\n",
    "            if page_num > cur_page_num:\n",
    "                group = url[0:url.rfind('/')]\n",
    "                print(group)\n",
    "                try:\n",
    "                    get_all_series(group+'/'+page['href'], recursecall=True)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "get_all_series('https://www.1001tracklists.com/groups/index.html')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
